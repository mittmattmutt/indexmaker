# indexmaker

# Intro

The aim of indexmaker is to help make indexes for books. It produces what I'll call an *index skeleton*, by which I mean something that you can use as a first-step in making indexes for books, a time-consuming and boring task. An index skeleton isn't an index: to turn the former into the latter will take subject-specific expertise and plenty of tweaking. But my hope is that it will help anyone resource-poor who finds themselves needing to make an index for a book without the time/money/patience to do so. It also includes a GUI, in the form of an html file, in which the index skeleton is displayed, and can be edited and searched.

In more detail, an IS (I will abbreviate it because taking about skeletons all the time seems a bit morbid) consists of three sorts of items: a list of proper names and the places they occur in a given input book, a list of adjectives and noun phrases containing that adjective where the adjective both occurs frequently in the input book and infrequently elsewhere, and a list of nouns and noun phrases such that the noun both occurs frequently in the input book and infrequently elsewhere. Here, for example, is an extract for the IS generated by IM for the academic philosophy book [*Conceptual Engineering and Conceptual Ethics*](https://global.oup.com/academic/product/conceptual-engineering-and-conceptual-ethics-9780198801856?cc=gb&lang=en&), a book on which I happened to work as research assistant (and a chunk of which is included, with permission, in this package as a guide):

>AMELIORATIVE
ameliorative: 43,147,
ameliorative analysis: 107,
ameliorative concept: 111,111,
ameliorative definition: 116,
ameliorative definition—and: 116,
ameliorative project: 102,147,
ameliorative spirit: 132,132,132,132,
ameliorative strategy: 141,

>BIOLOGICAL
biological: 108,
biological category: 107,
biological concept: 54,107,
biological construal: 109,110,
biological determinism: 108,108,
biological fact: 53,
biological realism: 108,
biological role: 36,104,111,
biological sex: 105,107,109,109,109,121,
biological view: 53,
>...
>
>BELLERI
>belleri: 76,
>delia belleri: 59,

>BENNETT
>bennett: 70,72,76,
>karen bennett: 70,

This is good! Both the notions of biology and amelioration play important roles in the book, and if one assumes that the any name occurring in a text ought to be indexed, so should Belleri and Bennett, and we see the ISM groups together occurrences of either just the surname (frequent in academic contexts) and both names (incidentally, already here I can point out one of the many issues with the current version: if two people share a surname, they'll both be categorized under the same entry, something you'll have to fix manually).

However, it's not an index. It needs editing. You need to know why exactly biology is important in the text in question. You also need to look to see if other terms might be relevant. For example, a bit later we see

>GENDER
>certain gender: 117,119,120,
>female gender: 112,112,112,113,
>gender-appropriate social recognition: 113,117,
>gender-based discrimination: 106,107,115,116,117,117,118,
>gender-related norm: 112,
>genderappropriate recognition: 113,
>other gender: 117,118,
>own particular gender: 118,
>particular gender: 113,117,118,119,119,
>public gender: 111,
>s gender: 109,
>second gender: 118,
>social-psychological gender: 112,
>such social gender: 116,

So it seems gender is important. But are gender and biology together important themes? Well, you'll just have to work that out. And incidentally, we see here some further issues: the script includes unhelpful terms like 's gender' and 'such social gender'. You'll just have to delete them yourself.

Moving on, perhaps with some tidying, an entry might read:

> **Biology**
>Biological sex
>biological determinism
>*see also* gender

Now this looks like a reasonable index item! You'll also get useless nonsense like:

>STAND
>beliefs stand: 84,
>conventional understanding: 126,
>general understanding: 137,
>natural understanding: 45,
>philosophical understanding: 61,
>proper understanding: 107,
>standard: 65,84,43,68,83,97,117,
>standard accounts: 83,
>standard case: 138,
>standard concept: 82,
>standard conception: 51,
>standard externalist: 148,
>standard fictionalist: 65,
>standard intensional individuation: 83,
>standard usage: 105,138,
>standard worm-theory perdurantist: 71,
>standing: 84,
>understand: 45,
>understand backwards: 58,
>understanding: 37,48,48,92,115,147,

The hope is that the useful non-nonsense outweighs the useless nonsense. I am not certain whether that is true. I'm pretty confident the GUI should be useful, however.

# Should I use this?

I'm not sure. If you have, or can easily get, a version of your text in .txt file, I think it would do no harm. If it's hard to get such a version, I would test it out first--convert and clean a small amount of text and see if it looks roughly okay. There are many disciplines for which it might work very poorly (anything with lots of numbers or non-Latin characters, for example). It should work okay hopefully for disciplines that like to use a lot of weird jargon, especially if those whose members are prone to nominalizing that jargon.

# Set-up

IM is a python script and you'll need python installed on your computer to run it. I'm not sure which all versions it supports, but 3.6 at least. If you already have python, you need to get also the [Natural Language Toolkit](https://www.nltk.org/) for Python (NLKT) and IM uses at least one corpus so you should download them too (you can just pip it, if you know what that is, else follow the link I just gave). One of the key requirements is that you have your book in text format--so, notably, *not* pdf. This is a big chokepoint: converting from pdf to txt is a bit perilous, and the ease with which you can do so will depend on idiosyncracies of the typeset text the publisher gives you. I use xpdf-tools pdftotext command line tool--I can vouch that it works with Oxford University Press documents. So, to recap:

1) Python, definitely
2) NLKT and corpora, definitely
3) A pdf to text tool, preferably
4) A web-browser, with internet access (the latter is strictly speaking not necessary--if you want to use it offline, download bootstrap and point to the local version using html <style> tags.

And, of course,

5) indexskeletonmaker.py

The easiest thing to do is just download this whole repository to a directory. You can then quickly get going by making sure your txt-formatted book (let's call the file book.txt) is in the directory, and simply typing the below (or amending slightly the file make.py included here), where x and y are non optional parameters that specify the first and the last page to be indexed. 

`import indexskeletonmaker

indexmaker.quick_make("book.txt",[x,y]))`

This should create two files: index.js and text.js. If you open them, you'll see that they are simply define variables corresponding to the text divided into page-size chunks, and the index. They are used as ways for the html editor to read the text. The next step is open edit.html, and again all being well, you should see the index, and be able to navigate through it, add and remove items, and download it in a format you can use to turn the skeleton into a proper index.

# In Lieu Of Documentation

An index skeleton is a list of lists. Let's call entries in the index, well, entries. The first entry is:

['ameliorative', [['ameliorative', [[43, 147]]], ['ameliorative analysis', [[107]]], ['ameliorative concept', [[111, 111]]], ['ameliorative definition', [[116]]], ['ameliorative definition—and', [[116]]], ['ameliorative project', [[102, 147]]], ['ameliorative spirit', [[132, 132, 132, 132]]], ['ameliorative strategy', [[141]]]]]

Each entry is an array consisting of a heading, which is a string, and an list of lists that record complex phrases formed from the heading (or, in mandatory first place, the phrase itself) along with the locations that complex phrase (/the phrase itself) occurs in the text. Locations are stored in a list-inside-a-list (why not just a list? I can't remember--it might be a mistake, or maybe there was some reason for it I've forgotten, and I can't be bothered working out which and/or fixing it right now).

Entries are generated on the basis of a text-format book in a series of steps. First of all, function **text_parser()** is called, which takes a filename, and an array consisting of numbers of the first and last pages, and returns a list of two element lists the first of which is the page number and the second of which is the text on that page.

In its current form, this function is extremely brittle. Check out sample.txt to see how the text **must** be presented. The thing to note is that you want to have the page numbers, then the text, then the page number of the next file. Note that if your text is unlucky to contain the numeral which denotes n+1 on page n, this function will break. This is easily fixed and I will maybe fix it in future should I need to. I would save everything as ansi rather than utf-8, which gives me errors.

Then the heavy lifting occurs, with function **taggingparsed()**. It's here we make most use of NLKT. We loop through the pages, and tag and parse them in acccordance with a grammar we specify. Our grammar looks for strings containing a number of adjectives followed by a noun, or strings that NLKT thinks denote people. When it finds one them, it writes it to a new file. Here are the first several items of the test book:

['(NP Revisionary/JJ Analysis/NN)', 35], ['(NP Or/NNP)', 35], ['(NP (PERSON Could/NNP Women/NNP Be/NNP Analytically/NNP))', 35], ['(NP (PERSON Derek/NNP Ball/NNP))', 35], ['(NP number/NN)', 35], ['(NP philosophical/JJ analysis/NN)', 35], ['(NP common/JJ thread/NN)', 35], ['(NP aim/NN)', 35], ['(NP analysis/NN)', 35], ['(NP philosophical/JJ conclusion/NN)', 35]

This is the slowest part of the process. If you have the background (or are willing to learn; there's nothing fancy here) and want to play about, I would recommend writing these intermediate outputs to files, then reading them (I use ast.string_literal_eval) and applying the next function in the process to them. At this point, we split off, and do one of two things, depending on whether we want to collect up names, or important phrases. If the former, we feed the above to **namecount()**. This collects all the trees marked PERSON, along the way counting up how many occurrences of each such tree there is. It then de-trees then, so for example it turns '(NP (PERSON Derek/NNP Ball/NNP))' to 'Derek Ball'. Why? Well, I'm not sure. I probably should have held off doing this bit til later.

Turn now to non-PERSON-tagged trees. We do basically the same thing as above, with the function **occurrencecount()** by converting the list into a dict the keys of which are the phrases, to the values of which are appended page numbers. occurrencecount also--for reasons of incompetence--converts the above phrase structure representations of expressions into non-tagged English, yielding output like:

['a', [46, 46, 76, 76, 77, 80, 93, 106, 110, 126, 130, 130, 130, 130, 130, 131, 131, 131, 131, 131, 132, 134, 138, 140, 141]], ['a plea', [139]], ['a priori', [57]], ['a ’', [131]], ['a.', [57, 57, 57, 76]], ['a. l.', [59, 77, 77, 77, 77]], ['absent', [71]], ['actually', [96]], ['adoption grasp', [47, 48]], ['adoption grasp', [48]], ['adoption grasp a', [46]], ['advertisement', [57]], ['again', [140, 147]], ['alcoff', [105, 114, 116]], ['alexis burgess', [66, 75, 75, 100, 125, 141]]

Not looking great! There's lots of garbage here that got pulled in by the various routines. We can tidy it up considerably with the functions **categorize_nouns()**, **categorize_adjs()**, and the function **categorizeav()** which basically just runs the two former functions. What we do in the first two is look for adjectives and nouns occurring in phrases that both occur relatively commonly (more than 7, but that number can and should be changed depending on the size of the document--look at lines 204 and 237 of indexmaker.py). **Categoriseav()** provides another filter, chucking out any phrases with fewer than 2 characters and which occur less than 18 times in a (more or less randomly chosen corpus). I encourage again experimentation with these numbers and corpora, but this seems to work okay for the test file I used.

At this point, things are looking okay--you'll get about like that quoted at the start of this section. We then concatenate the lists containing the names and the non-name-phrases, sort in alphabetical order, write to the .js files. At that point, you should open edit.html, and the index should appear ready for inspection and the addition and deletion of items.

# Licence

MIT. If you improve upon this, I would love to know. The code is a super horrible mess (so: [please](https://i.kym-cdn.com/photos/images/original/001/214/092/3d9.jpg)). I would also love to know if you find mistakes.


