# indexmaker

# Intro

The aim of indexmaker is to help make indexes for books. It produces what I'll call an *index skeleton*, by which I mean something that you can use as a first-step in making indexes for books, a time-consuming and boring task. An index skeleton isn't an index: to turn the former into the latter will take subject-specific expertise and plenty of tweaking. But my hope is that it will help anyone resource-poor who finds themselves needing to make an index for a book without the time/money/patience to do so.

In more detail, an IS (I will abbreviate it because taking about skeletons all the time seems a bit morbid) consists of three sorts of items: a list of proper names and the places they occur in a given input book, a list of adjectives and noun phrases containing that adjective where the adjective both occurs frequently in the input book and infrequently elsewhere, and a list of nouns and noun phrases such that the noun both occurs frequently in the input book and infrequently elsewhere. Here, for example, is an extract for the IS generated by IM for the academic philosophy book [*Conceptual Engineering and Conceptual Ethics*](https://global.oup.com/academic/product/conceptual-engineering-and-conceptual-ethics-9780198801856?cc=gb&lang=en&), a book on which I happened to work as research assistant:

>AMELIORATIVE
>ameliorative: 43,147,
>ameliorative analysis: 107,
>ameliorative concept: 111,111,
>ameliorative definition: 116,
>ameliorative definition—and: 116,
>ameliorative project: 102,147,
>ameliorative spirit: 132,132,132,132,
>ameliorative strategy: 141,

>BIOLOGICAL
>biological: 108,
>biological category: 107,
>biological concept: 54,107,
>biological construal: 109,110,
>biological determinism: 108,108,
>biological fact: 53,
>biological realism: 108,
>biological role: 36,104,111,
>biological sex: 105,107,109,109,109,121,
>biological view: 53,
>...
>
>BELLERI
>belleri: 76,
>delia belleri: 59,

>BENNETT
>bennett: 70,72,76,
>karen bennett: 70,

This is good! Both the notions of biology and amelioration play important roles in the book, and if one assumes that the any name occurring in a text ought to be indexed, so should Belleri and Bennett, and we see the ISM groups together occurrences of either just the surname (frequent in academic contexts) and both names (incidentally, already here I can point out one of the many issues with the current version: if two people share a surname, they'll both be categorized under the same entry, something you'll have to fix manually).

However, it's not an index. It needs editing. You need to know why exactly biology is important in the text in question. You also need to look to see if other terms might be relevant. For example, a bit later we see

>GENDER
>certain gender: 117,119,120,
>female gender: 112,112,112,113,
>gender-appropriate social recognition: 113,117,
>gender-based discrimination: 106,107,115,116,117,117,118,
>gender-related norm: 112,
>genderappropriate recognition: 113,
>other gender: 117,118,
>own particular gender: 118,
>particular gender: 113,117,118,119,119,
>public gender: 111,
>s gender: 109,
>second gender: 118,
>social-psychological gender: 112,
>such social gender: 116,

So it seems gender is important. But are gender and biology together important themes? Well, you'll just have to work that out. And incidentally, we see here some further issues: the script includes unhelpful terms like 's gender' and 'such social gender'. You'll just have to delete them yourself.

Moving on, perhaps with some tidying, an entry might read:

>#Biology
>Biological sex
>biological determinism
>*see also* gender

Now this looks like a reasonable index item! You'll also get useless nonsense like:

>STAND
>beliefs stand: 84,
>conventional understanding: 126,
>general understanding: 137,
>natural understanding: 45,
>philosophical understanding: 61,
>proper understanding: 107,
>standard: 65,84,43,68,83,97,117,
>standard accounts: 83,
>standard case: 138,
>standard concept: 82,
>standard conception: 51,
>standard externalist: 148,
>standard fictionalist: 65,
>standard intensional individuation: 83,
>standard usage: 105,138,
>standard worm-theory perdurantist: 71,
>standing: 84,
>understand: 45,
>understand backwards: 58,
>understanding: 37,48,48,92,115,147,

The hope is that the useful non-nonsense outweighs the useful nonsense. I am not certain whether that is true.

# Set-up

IM is a python script and you'll need python installed on your computer to run it. I'm not sure which all versions it supports, but 3.6 at least. If you already have python, you need to get also the [Natural Language Toolkit](https://www.nltk.org/) for Python (NLKT) and IM uses at least one corpus so you should download them too (you can just pip it, if you know what that is, else follow the link I just gave). One of the key requirements is that you have your book in text format--so, notably, *not* pdf. This is a big chokepoint: converting from pdf to txt is a bit perilous, and the ease with which you can do so will depend on idiosyncracies of the typeset text the publisher gives you. I use xpdf-tools pdftotext command line tool--I can vouch that it works with Oxford University Press documents. So, to recap:

1) Python, definitely
2) NLKT and corpora, definitely
3) A pdf to text tool, preferably

And, of course,

4) indexskeletonmaker.py

You can then quickly get going by making sure 4) is in a directory with your txt-formatted book (let's call the file book.txt), and simply typing the below, where x and y are non optional parameters that specify the first and the last page to be indexed. All being well, you should see a moderately nicely formatted looking index.

import indexskeletonmaker

print(indexmaker.quick_make("texts/big.txt",[x,y]))

In Lieu Of Documentation

An index skeleton is a list of lists. Let's call entries in the index, well, entries. The first entry is:

['ameliorative', [['ameliorative', [[43, 147]]], ['ameliorative analysis', [[107]]], ['ameliorative concept', [[111, 111]]], ['ameliorative definition', [[116]]], ['ameliorative definition—and', [[116]]], ['ameliorative project', [[102, 147]]], ['ameliorative spirit', [[132, 132, 132, 132]]], ['ameliorative strategy', [[141]]]]]

Each entry is an array consisting of a heading, which is a string, and an list of lists that record complex phrases formed from the heading (or, in mandatory first place, the phrase itself) along with the locations that complex phrase (/the phrase itself) occurs in the text. Locations are stored in a list-inside-a-list (why not just a list? I can't remember--it might be a mistake, or maybe there was some reason for it I've forgotten, and I can't be bothered working out which and/or fixing it right now).

Entries are generated on the basis of a text-format book in a series of steps. First of all, function text_parser is called, which takes a filename, and an array consisting of numbers of the first and last pages, and returns a list of two element lists the first of which is the page number and the second of which is the text on that page.

Then the heavy lifting occurs, with function taggingparsed. It's here we make most use of NLKT. We loop through the pages, and tag and parse them in acccordance with a grammar we specify. Our grammar looks for strings containing a number of adjectives followed by a noun, or strings that NLKT thinks denote people. When it finds one them, it writes it to a new file. Here are the first several items of the test book:

['(NP Revisionary/JJ Analysis/NN)', 35], ['(NP Or/NNP)', 35], ['(NP (PERSON Could/NNP Women/NNP Be/NNP Analytically/NNP))', 35], ['(NP (PERSON Derek/NNP Ball/NNP))', 35], ['(NP number/NN)', 35], ['(NP philosophical/JJ analysis/NN)', 35], ['(NP common/JJ thread/NN)', 35], ['(NP aim/NN)', 35], ['(NP analysis/NN)', 35], ['(NP philosophical/JJ conclusion/NN)', 35]

The next thing we do is count up occurrences of these phrases. This is done in occurrencecount by converting the list into a dict the keys of which are the phrases, to the values of which are appended page numbers. occurrencecount also--for reasons of incompetence--converts the above phrase structure representations of expressions into non-tagged English, yielding output like:

['a', [46, 46, 76, 76, 77, 80, 93, 106, 110, 126, 130, 130, 130, 130, 130, 131, 131, 131, 131, 131, 132, 134, 138, 140, 141]], ['a plea', [139]], ['a priori', [57]], ['a ’', [131]], ['a.', [57, 57, 57, 76]], ['a. l.', [59, 77, 77, 77, 77]], ['absent', [71]], ['actually', [96]], ['adoption grasp', [47, 48]], ['adoption grasp', [48]], ['adoption grasp a', [46]], ['advertisement', [57]], ['again', [140, 147]], ['alcoff', [105, 114, 116]], ['alexis burgess', [66, 75, 75, 100, 125, 141]]

Not looking great! There's lots of garbage here that got pulled in by the various routines.


